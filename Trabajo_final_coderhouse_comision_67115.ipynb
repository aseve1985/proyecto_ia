{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Final: \n",
    "\n",
    "Generación automática de consultas SQL y respuestas ad hoc sobre un ecosistema de datos.\n",
    "El objetivo del proyecto es dar solución a una problemática que sucede en las empresas con los modelos de datos y su entendimiento. Para los usuarios experimentados es terreno conocido, pero para los usuarios nuevos la documentación suele ser poco amigable y la curva de aprendizaje se extiende en el tiempo. Este proyecto está orientado a acortar esta curva de aprendizaje y a acercar la información de las tablas a las personas y sus consultas.\n",
    "\n",
    "## Índice\n",
    "1. Introducción\n",
    "2. Objetivos\n",
    "3. Metodología\n",
    "4. Herramientas y Tecnologías\n",
    "5. Implementación\n",
    "6. Resultados\n",
    "7. Conclusiones\n",
    "8. Referencias\n",
    "\n",
    "## 1. Introducción\n",
    "En el contexto empresarial, un desafío común al que se enfrentan los gerentes o jefes de Datos es la dificultad de transmitir de manera eficiente el conocimiento sobre el modelo de datos de la organización a nuevos analistas, ya sea recién ingresados al equipo o internos que necesitan familiarizarse con el esquema de datos.\n",
    "Los modelos de datos suelen ser extensos y complejos, con nomenclaturas poco intuitivas que dificultan su comprensión inicial. Esta curva de aprendizaje puede extenderse por meses, ralentizando la productividad de los analistas y generando ineficiencias en la operación.\n",
    "Acelerar el entendimiento del modelo de datos, la construcción de consultas para obtener información específica y el conocimiento sobre la composición de campos, filas y calidad de la información representaría un valor significativo para la organización.\n",
    "\n",
    "## 2. Objetivos\n",
    "El objetivo principal es acelerar el entendimiento de un modelo de datos y facilitar el acceso a información clave. Aplica a perfiles técnicos que precisen conocer el modelo y perfeccionar sus querys aplicándolas a casuísticas reales, o bien a usuario no técnicos que tienen preguntas concretas sobre la información de la empresa y que quieren entender las respuestas y también su vía de obtención.\n",
    "\n",
    "## 3. Metodología\n",
    "- Conexión con la base de datos: Conexiones vía Python a un set de datos.\n",
    "- Creación de las consultas SQL: Utilización de GPT 3.5 para el entendimiento de la pregunta del requirente y su traducción a SQL.\n",
    "- Conversión a lenguaje natural: Utilización de GPT 3.5 para la traducción del resultado del SQL a un lenguaje natural coloquial.\n",
    "- Iteración: Funciones de contexto acumulado y preguntas encadenadas para lograr iteración y repreguntas.\n",
    "\n",
    "## 4. Herramientas y Tecnologías\n",
    "- Kernel: Python\n",
    "- AI: gpt-3.5-turbo\n",
    "- Librerías utilizadas: psycopg2 openai pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías necesarias\n",
    "from psycopg2 import connect\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo de lenguaje\n",
    "openai.api_key = 'xxxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configurar conexión a Redshift\n",
    "def conectar_redshift():\n",
    "    \"\"\"\n",
    "    Crea y devuelve una conexión a Redshift.\n",
    "    \"\"\"\n",
    "    return connect(\n",
    "    dbname='xxxxx',\n",
    "    user='xxxxx',\n",
    "    password='xxxxx',\n",
    "    host='xxxxx',\n",
    "    port='xxxxx'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Lista global para almacenar variables\n",
    "contexto_acumulado = [] \n",
    "\n",
    "pregunta_usuario = []\n",
    "\n",
    "pregunta_siguiente = []\n",
    "\n",
    "sql_generado = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contexto detallado del promt inicial\n",
    "contexto = \"\"\"\n",
    "            Usa tablas relevantes según la pregunta. Respeta siempre las convenciones de los roles específicos.Eres un experto en bases de datos SQL, especializado en sistemas complejos como Redshift. \n",
    "            Tu tarea es convertir preguntas de usuarios no técnicos en consultas SQL eficientes y optimizadas, \n",
    "            teniendo en cuenta el esquema y las relaciones entre tablas, así como las mejores prácticas en SQL. Debes tener en cuenta las siguientes reglas:\n",
    "\n",
    "            Posibles paises en los que tenemos operacion: Argentina y Colombia.\n",
    "\n",
    "            Para consultas sobre Colombia, usa las tablas que terminan en _col. adicionalmente los schemas sobre los que hacer las consultas tambein terminan en _col.\n",
    "            Para consultas sobre Argentina, usa las tablas que terminan en _arg. adicionalmente los schemas sobre los que hacer las consultas tambein terminan en _arg.\n",
    "\n",
    "            Para entender las conexiones entre las tablas el concepto principal es que las tablas tienen un ID y se vinculan con otra tabla de referencia. Por ejemplo, la tabla LEADS tiene un ID, y todas las tablas que tengan el campo LEAD_ID joinean contra ella. \n",
    "            \n",
    "            Las tablas relevantes para entender de las ventas de los creditos son:\n",
    "\n",
    "            Argentina: core_arg.loans_arg\n",
    "            Colombia: core_col.loans_col\n",
    "            \n",
    "            En donde los campos mas relevantes para entender los requerimientos de los usuarios son diferenes para cada pais, en el caso de Argentina son:\n",
    "\n",
    "            date(payment_date): es la fecha de venta o desembolso del credito. Siempre debemos ponerle la funcion date() previamente para convertirlo en fecha.\n",
    "            date(created_at): es la fecha de creacion del registro en la tabla. Siempre debemos ponerle la funcion date() previamente para convertirlo en fecha.\n",
    "            is_renovation: true o 1 indica si el cliente es renovador y false o 0 cuando es nuevo.\n",
    "            term: es el plazo del credito.\n",
    "            amount: es el monto o capital vendido\n",
    "            installment_amount: es el monto que tiene que pagar el cliente por mes, es decir la cuota.\n",
    "            el interes del credito puede ser calculado multiplicando el installment_amount * el term y restandole a ello el amount.\n",
    "            rate: es un dato de clasificacion importante que se llama rate.\n",
    "            lead_id: es un campo que permite el join contra otra tabla importante core_arg.leads_arg via el campo core_arg.leads_arg.id\n",
    "            \n",
    "            Es importante para entender la venta de argentina que solo hay que tomar los registros que tengan status_id in (20,39), el resto son ventas anuladas.\n",
    "\n",
    "            En el caso de colombia los campos relevantes son similares, perolevemente diferentes:\n",
    "\n",
    "            date(transferred_date): es la fecha de venta o desembolso del credito. Siempre debemos ponerle la funcion date() previamente para convertirlo en fecha.\n",
    "            date(created_at): es la fecha de creacion del registro en la tabla.Siempre debemos ponerle la funcion date() previamente para convertirlo en fecha.\n",
    "            is_renovation: true o 1 indica si el cliente es renovador y false o 0 cuando es nuevo.\n",
    "            terms: es el plazo del credito.\n",
    "            total_amount: es el monto o capital vendido\n",
    "            installment_amount: es el interes total que paga el cliente.\n",
    "            rate: es un dato de clasificacion importante que se llama rate.\n",
    "            lead_id: es un campo que permite el join contra otra tabla importante core_arg.leads_arg via el campo core_col.leads_col.id\n",
    "\n",
    "            Es importante para entender la venta de colombia que solo hay que tomar los registros que tengan status_id in (22,24), el resto son ventas anuladas.\n",
    "           \n",
    "            Por otro lado, las tablas relevantes de cobranzas son las siguientes:\n",
    "            Argentina: collections_arg.cobranza_arg\n",
    "            Colombia: collections_col.cobranza_col\n",
    "            \n",
    "            Por otro lado, las tablas relevantes a \"Pegadas del motor\" o \"consutlas del motor\" o \"Consultas de riesgos\" o \"consutlas de uflow\" son las siguientes:\n",
    "            \n",
    "            Argentina: gold.consultas_motor_arg\n",
    "            Esta tablas se pueden unir contra la tabla leads mediante el ik id y partiendo de la columna external_id cuando el external_type = 'lead_id', este join solo debe hacerse si se quiere obtener informacion adicional, sino no es necesario para pbtener data exclusiva de las consultas al motor de riesgo.\n",
    "            En esta tabla el ID unico es: executionid\n",
    "            El campo de fecha de consulta es: executiondate_date\n",
    "            La decision del motor, el estado de la consulta si es aprobado, rechazado o error,  la obtenes con el siguiente campo: decisionresult_lite este es el estado de la consulta, la resolucion del motor. \n",
    "            la fuente se obtiene del campo: Fuente\n",
    "            El nivel de riesgo se obtiene del campo: perfil_riesgo\n",
    "            El nro de documento de las personas se encuentra en el campo: nrodoc.\n",
    "            \n",
    "-- \n",
    "            Colombia: risk_col.risk_engine_col\n",
    "            Esta tablas se pueden unir contra la tabla leads mediante el ik id y partiendo de la columna lead_id. , este join solo debe hacerse si se quiere obtener informacion adicional, sino no es necesario para pbtener data exclusiva de las consultas al motor de riesgo.\n",
    "            En esta tabla el ID unico es: executionid\n",
    "            El campo de fecha de consulta es: date(executiondate)\n",
    "            La decision del motor la obtenes con el siguiente case: 'case \n",
    "                                                                        when upper(split_part(rec.decisionresult,' ','1')) = 'LEAD' then 'APROBADO'\n",
    "                                                                        when upper(split_part(rec.decisionresult,' ','1')) = 'APROBADO' then 'APROBADO'\n",
    "                                                                        when upper(split_part(rec.decisionresult,' ','1')) = 'RECHAZO' then 'RECHAZADO'\n",
    "                                                                        when upper(split_part(rec.decisionresult,' ','1')) = 'RECHAZADO' then 'RECHAZADO'\n",
    "                                                                        when upper(split_part(rec.decisionresult,' ','1')) in ('ERROR', 'NO') then 'ERROR'\n",
    "                                                                        when rec.decisionresult is null or rec.decisionresult = '' then 'DECISION_EN_BLANCO'\n",
    "                                                                        else 'INVESTIGAR' end estado_consulta'\n",
    "            la fuente se obtiene del campo: Fuente\n",
    "            El nivel de riesgo se obtiene del campo: perfil_riesgo\n",
    "            El nro de documento de las personas se encuentra en el campo: Cedula.\n",
    "            \n",
    "            Si se dean obtener datos de columnas referentes a 'mareigua' el join con las tablas es el siguiente:\n",
    "            left join risk_col.risk_engine_2_col rec2 \n",
    "            on risk_col.risk_engine_col.executionid = rec2.executionid_2        \n",
    "            \n",
    "            A nivel general y para todas las consultas de SQL a generar, para una mejor performance NO utilizar DATE_TRUNC, mejor usar EXTRACT o DATE_PART o bien between de fechas.\n",
    "            Tambien es importante que a todos los campos de FECHAS antes le agreguemos la funcion date(), porque muchos de ellos tienen el formato equivocado.\n",
    "            Por otro lado como mejor practica, NUNCA utilices en los group by el alias del campo, siempre utiliza el campo en si, ya sea tal como existe, o la construcciond el campo en si mismo.\n",
    "            \n",
    "            Asegúrate de generar consultas seguras y optimizadas, respetando el contexto geográfico de las tablas y las relaciones de la base de datos.\n",
    "            \n",
    "            Lo mas importante, cuando entregues el resultado, solo entrega la consulta SQL en sintaxis de AWS redshift, debes validar que la sintaxis de la query sea correcta y que no falle su ejecucion, \n",
    "            ademas, no agregues redaccion al resultado, ya que con tu consulta de SQL luego voy a ejecutar en base de datos para obtener el resultado.\n",
    "            \n",
    "            Convierte la siguiente pregunta en una consulta SQL:\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto_acumulado.append(contexto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_sql_con_ia(pregunta_usuario, contexto_acumulado):\n",
    "    \"\"\"\n",
    "    Utiliza OpenAI para generar una consulta SQL basada en la pregunta del usuario.\n",
    "    Se adapta según el rol del usuario, utilizando solo tablas y campos relevantes definidos.\n",
    "    \"\"\"\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "            Eres un asistente experto en SQL para bases de datos Amazon Redshift. Genera consultas SQL basadas en preguntas del usuario, usando exclusivamente las tablas y campos proporcionados. \n",
    "            Contexto del sistema: {contexto_acumulado}\n",
    "            Devuelve solo la consulta SQL, sin explicaciones adicionales.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Pregunta: {pregunta_usuario}\\n\\nConsulta SQL:\"}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Llamada al modelo de OpenAI\n",
    "        respuesta = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  \n",
    "            messages=conversation,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        # Limpiar la salida del modelo\n",
    "        sql_generado = respuesta.choices[0].message['content'].strip()\n",
    "\n",
    "        if sql_generado.startswith(\"```sql\"):\n",
    "            sql_generado = sql_generado[6:]  # Eliminar ```sql\n",
    "        if sql_generado.endswith(\"```\"):\n",
    "            sql_generado = sql_generado[:-3]  # Eliminar ```\n",
    "        \n",
    "        # Remover saltos de línea\n",
    "        sql_generado = sql_generado.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "        # Actualizar el contexto acumulativo\n",
    "        contexto_acumulado.append({\"pregunta\": pregunta_usuario, \"sql\": sql_generado})\n",
    "\n",
    "        return sql_generado.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar SQL: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_consulta_redshift(sql_generado):\n",
    "    \"\"\"\n",
    "    Ejecuta una consulta SQL en Amazon Redshift y devuelve los resultados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conexion = conectar_redshift()\n",
    "        with conexion.cursor() as cursor:\n",
    "            cursor.execute(sql_generado)\n",
    "            columnas = [desc[0] for desc in cursor.description]  # Obtener nombres de columnas\n",
    "            resultados = [dict(zip(columnas, fila)) for fila in cursor.fetchall()]\n",
    "            \n",
    "        conexion.close()\n",
    "        return resultados, contexto_acumulado.append(resultados)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al ejecutar la consulta: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformar_a_lenguaje_natural(resultados, pregunta_usuario, contexto_acumulado):\n",
    "\n",
    "    if not resultados:\n",
    "        return \"No se encontraron resultados para tu consulta.\"\n",
    "    \n",
    "    try:\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente que convierte datos tabulares en respuestas claras y amigables. tu rol principal es explicar los resultados a una persona que quiere entender lo que ha preguntado\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "            Contexto: La consulta SQL devolvió estos resultados: {resultados}.\n",
    "            Responde de manera profesional y clara basándote en la pregunta: \"{pregunta_usuario}\".\n",
    "            \"\"\"}\n",
    "        ]\n",
    "        respuesta = openai.ChatCompletion.create(\n",
    "            model = \"gpt-3.5-turbo\",\n",
    "            messages=conversation,\n",
    "            max_tokens = 1000,\n",
    "            temperature = 0.5,\n",
    "        )\n",
    "        \n",
    "                # Actualizar el contexto acumulativo con la respuesta\n",
    "        contexto_acumulado.append(respuesta.choices[0].message['content'].strip())\n",
    "        \n",
    "        return respuesta.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al transformar resultados: {e}\")\n",
    "        return \"Hubo un error al generar la respuesta en lenguaje natural.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Función principal para procesar la pregunta\n",
    "def procesar_pregunta(pregunta_usuario):\n",
    "    \"\"\"\n",
    "    Procesa la pregunta del usuario y devuelve una respuesta en lenguaje natural.\n",
    "    \"\"\"\n",
    "    # Paso 1: Generar la consulta SQL\n",
    "    consulta_sql = generar_sql_con_ia(pregunta_usuario, contexto_acumulado=contexto_acumulado)\n",
    "    if not consulta_sql:\n",
    "        return \"No entendí la consulta, ¿Podés reformularla?\"\n",
    "    \n",
    "    # Imprimir la consulta SQL generada\n",
    "    print(f\"Gracias por tu consulta: * {pregunta_usuario} * \\n. Generé la siguiente consulta SQL con la cual la voy a responder:\\n {consulta_sql}\")\n",
    "    \n",
    "    # Paso 2: Ejecutar la consulta en Redshift\n",
    "    resultados = ejecutar_consulta_redshift(consulta_sql)\n",
    "    if not resultados:\n",
    "        return \"Hubo un error al ejecutar la consulta en Redshift.\"\n",
    "    \n",
    "    # Paso 3: Transformar los resultados en una respuesta coloquial\n",
    "    respuesta = transformar_a_lenguaje_natural(resultados, pregunta_usuario, contexto_acumulado=contexto_acumulado)\n",
    "    \n",
    "    print(respuesta)\n",
    "    \n",
    "    return respuesta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_integrada():\n",
    "    pregunta_usuario = input(\"¿Qué te interesa consultar sobre nuestra empresa? \")\n",
    "    procesar_pregunta(pregunta_usuario)\n",
    "    \n",
    "    contexto_acumulado.append(pregunta_usuario)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_siguiente():\n",
    "    pregunta_siguiente = input(\"¿Qué te interesa saber? \")\n",
    "    procesar_pregunta(pregunta_siguiente)\n",
    "    \n",
    "    contexto_acumulado.append(pregunta_siguiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_encadenada():\n",
    "    while True:\n",
    "        resultado = consulta_integrada()\n",
    "        \n",
    "        if resultado != \"Hubo un error al ejecutar la consulta en Redshift.\":\n",
    "            \n",
    "            \n",
    "            while True:\n",
    "                continuar = input(\"¿Deseás realizar otra consulta? (si/no): \").strip().upper()\n",
    "                if continuar == 'SI':\n",
    "                    consulta_siguiente()\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Deteniendo la ejecución de la consulta.\")    \n",
    "                    break\n",
    "            \n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            reformular = input(\"Hubo un error al ejecutar la consulta. ¿Desea reformularla? (si/no): \").strip().lower()\n",
    "            if reformular.upper != 'SI':\n",
    "                print(\"Finalizando el proceso.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracias por tu consulta: * ¿Cuanto vendimos en argentina en el 2024 en cantidad y monto por mes? * \n",
      ". Generé la siguiente consulta SQL con la cual la voy a responder:\n",
      " SELECT DATE_TRUNC('month', date(payment_date)) AS month,        COUNT(*) AS quantity,        SUM(amount) AS total_amount FROM core_arg.loans_arg WHERE date(payment_date) >= '2024-01-01'   AND date(payment_date) < '2025-01-01'   AND status_id IN (20, 39) GROUP BY DATE_TRUNC('month', date(payment_date)) ORDER BY DATE_TRUNC('month', date(payment_date));\n",
      "Según los resultados de la consulta SQL, en Argentina en el año 2024 se vendió la siguiente cantidad y monto por mes:\n",
      "\n",
      "- Enero: Cantidad vendida: 1,759 unidades, Monto total: $130,446,544.00\n",
      "- Febrero: Cantidad vendida: 1,940 unidades, Monto total: $175,047,736.00\n",
      "- Marzo: Cantidad vendida: 1,704 unidades, Monto total: $175,066,000.00\n",
      "- Abril: Cantidad vendida: 1,464 unidades, Monto total: $155,185,000.00\n",
      "- Mayo: Cantidad vendida: 1,601 unidades, Monto total: $188,605,000.00\n",
      "- Junio: Cantidad vendida: 1,337 unidades, Monto total: $191,000,000.00\n",
      "- Julio: Cantidad vendida: 1,492 unidades, Monto total: $213,307,000.00\n",
      "- Agosto: Cantidad vendida: 2,476 unidades, Monto total: $296,965,000.00\n",
      "- Septiembre: Cantidad vendida: 3,542 unidades, Monto total: $459,200,000.00\n",
      "- Octubre: Cantidad vendida: 4,183 unidades, Monto total: $576,042,000.00\n",
      "- Noviembre: Cantidad vendida: 4,995 unidades, Monto total: $687,608,000.00\n",
      "- Diciembre: Cantidad vendida: 1,621 unidades, Monto total: $220,625,000.00\n",
      "\n",
      "Estos son los datos de ventas mensuales en cantidad y monto en Argentina durante el año 2024.\n",
      "Gracias por tu consulta: * podrias abrir esta venta entre renovadores y nuevos? * \n",
      ". Generé la siguiente consulta SQL con la cual la voy a responder:\n",
      " SELECT DATE_TRUNC('month', date(payment_date)) AS month,        is_renovation,        COUNT(*) AS quantity,        SUM(amount) AS total_amount FROM core_arg.loans_arg WHERE date(payment_date) >= '2024-01-01'   AND date(payment_date) < '2025-01-01'   AND status_id IN (20, 39) GROUP BY DATE_TRUNC('month', date(payment_date)), is_renovation ORDER BY DATE_TRUNC('month', date(payment_date)), is_renovation;\n",
      "Claro, los resultados de la consulta muestran la cantidad y el monto total de ventas mensuales divididas entre renovadores y nuevos clientes durante el año 2024. Aquí tienes un resumen de los datos:\n",
      "\n",
      "- En enero de 2024, hubo 1427 ventas de productos a nuevos clientes por un total de 108,611,601.00 y 332 ventas a renovadores por un total de 21,834,943.00.\n",
      "- En febrero de 2024, se realizaron 1504 ventas a nuevos clientes por un total de 136,336,765.00 y 436 ventas a renovadores por un total de 38,710,971.00.\n",
      "- En marzo de 2024, se registraron 1258 ventas a nuevos clientes por un total de 131,503,000.00 y 446 ventas a renovadores por un total de 43,563,000.00.\n",
      "- En abril de 2024, se efectuaron 1051 ventas a nuevos clientes por un total de 109,536,000.00 y 413 ventas a renovadores por un total de 45,649,000.00.\n",
      "- En mayo de 2024, se llevaron a cabo 1167 ventas a nuevos clientes por un total de 134,051,000.00 y 434 ventas a renovadores por un total de 54,554,000.00.\n",
      "- En junio de 2024, se realizaron 1015 ventas a nuevos clientes por un total de 142,864,000.00 y 322 ventas a renovadores por un total de 48,136,000.00.\n",
      "- En julio de 2024, se registraron 1079 ventas a nuevos clientes por un total de 152,303,000.00 y 413 ventas a renovadores por un total de 61,004,000.00.\n",
      "- En agosto de 2024, se efectuaron 1536 ventas a nuevos clientes por un total de 175,170,000.00 y 940 ventas a renovadores por un total de 121,795,000.00.\n",
      "- En septiembre de 2024, se llevaron a cabo 1671 ventas a nuevos clientes por un total de 195,098,000.00 y 1871 ventas a renovadores por un total de 264,102,000.00.\n",
      "- En octubre de 2024, se realizaron 1804 ventas a nuevos clientes por un total de 217,366,000.00 y 2379 ventas a renovadores por un total de 358,676,000.00.\n",
      "- En noviembre de 2024, se registraron 1893 ventas a nuevos clientes por un total de 219,650,000.00 y 3102 ventas a renovadores por un total de 467,958,000.00.\n",
      "- En diciembre de 2024, se efectuaron 585 ventas a nuevos clientes por un total de 60,142,000.00 y 1036 ventas a renovadores por un total de 160,483,000.00.\n",
      "\n",
      "Estos datos muestran cómo se distribuyen las ventas entre renovadores y nuevos clientes a lo largo del año 2024.\n",
      "Gracias por tu consulta: * en terminos generales, que participacion tiene la venta de nuevos sobre el total de la venta? * \n",
      ". Generé la siguiente consulta SQL con la cual la voy a responder:\n",
      " SELECT is_renovation, COUNT(*) AS quantity, SUM(amount) AS total_amount FROM core_arg.loans_arg WHERE date(payment_date) >= '2024-01-01' AND date(payment_date) < '2025-01-01' AND status_id IN (20, 39) GROUP BY is_renovation;\n",
      "Basándonos en los resultados de la consulta SQL, podemos observar que la venta de nuevos (is_renovation: 0) representa una participación mayor en términos de cantidad, con un total de 15,990 unidades vendidas. Sin embargo, en términos de valor total, la venta de renovaciones (is_renovation: 1) tiene una participación mayor, con un monto total de $1,686,465,914.00 en comparación con los $1,782,631,366.00 de la venta de nuevos.\n",
      "\n",
      "Por lo tanto, en términos generales, la venta de renovaciones tiene una mayor participación en el total de ventas en comparación con la venta de nuevos.\n",
      "Deteniendo la ejecución de la consulta.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    consulta_encadenada()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resultados\n",
    "En los resultados se puede ver como el usuario accede a resultados explicados, pero también a la forma de extraer esta información de la base. Esto puede resultar muy útil ya que no solo \"Da de comer\" sino que también \"Enseña a pescar\".\n",
    "\n",
    "## 7. Conclusiones\n",
    "Con un contexto inicial robusto, que esté alineado a la evolución del set de datos de la empresa, esta herramienta podría ser la primera aproximación a la información de una unidad de negocios y luego convertirse en un pívot entre la data y las preguntas.\n",
    "\n",
    "## 8. Referencias\n",
    "En este ejemplo se consume datos de un set de datos de AWS(Redshift) pero las aplicaciones son infinitas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
